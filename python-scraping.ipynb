{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpzggwZ4Za8h"
   },
   "source": [
    "## Jokes\n",
    "\n",
    "For the first example, let's fetch the HTML code for the web page found at http://pun.me/pages/funny-jokes.php. There is a list of 35 jokes on this page. If you don't already have Google Chrome installed, take a moment to do that.\n",
    "\n",
    "Open the URL in Chrome and then open Chrome Developer Tools. To do this, click on the More Options icon in the top right hand corner of your Chrome window, then click *More Tools* then *Developer Tools*.\n",
    "\n",
    "![Open Dev Tools](assets/scraping-1.jpg)\n",
    "\n",
    "In the developer tools window, select the *Elements* tab, or select `Ctrl + Shift + I` Windows / `Command + Shift + I` Mac. In this tab, the HTML code for the web page is displayed. As you move your mouse pointer over the code, the relevant parts of the webpage will be highlighted. See if you can find the HTML code associated with the jokes themselves.\n",
    "\n",
    "![HTML Code](assets/scraping-2.png)\n",
    "\n",
    "You can also access the HTML related to the jokes by hovering over one with your mouse. Right-click and select \"Inspect\" and you will arrive at the relevant HTML code. \n",
    "\n",
    "![right-click-element](assets/inspect-element-right-click.gif)\n",
    "\n",
    "\n",
    "There is a lot going on in the HTML. As you drill down into the code, you will find a section of code that starts like this:\n",
    "\n",
    "```html\n",
    "<div style=\"float:left;width:100%;\">\n",
    "    <div class=\"content\">\n",
    "        <p>\n",
    "            Our most-liked jokes which are genuinely funny - this list of jokes has been hand selected and contain a variety of clever, clean and silly jokes so be prepared to laugh.\n",
    "        </p>\n",
    "        <ol>\n",
    "         <li>Today at the bank, an old lady asked me to help check her balance. So I pushed her over.</li>\n",
    "        <li>I bought some shoes from a drug dealer. I don't know what he laced them with, but I've been tripping all day.</li>\n",
    "```\n",
    "\n",
    "\n",
    "As you move your mouse pointer over that section of code the list of jokes itself is highlighted. In order to be able to parse the HTML code to extract the data, you will need to understand a little about the structure of HTML. If you are already familiar with HTML then you are all set, but if you are new to the language then there are a few things to be aware of.\n",
    "\n",
    "HTML is made up of elements. An element is constructed from tags. A tag looks like this: `<tagname>`. In the code above, you can see that there are `<div>` tags, `<p>` tags, an `<ol>` tag and some `<li>` tags. Each tag has a corresponding closing tag, which looks just like the tag but with a slash. So the `<li>` tag has a corresponding closing `</li>` tag.\n",
    "\n",
    "An `li` element is made up of the opening and closing `li` tags with some content between them like this:\n",
    "\n",
    "```html\n",
    "<li>Today at the bank, an old lady asked me to help check her balance. So I pushed her over.</li>\n",
    "```\n",
    "\n",
    "The element simply tells the browser what to display and maybe some information about the content. For instance, `<p>` is for a paragraph of text, `<ol>` is for ordered lists (that is, numbered lists), and `<li>` is a list item. Notice how the `<ol>` element contains a number of `<li>` elements. Notice also that each `<li>` element corresponds to a single numbered item in the list.\n",
    "\n",
    "There are a great many other elements on the page, but they are for headings, menus, and navigations that we are not interested in right now. You can typically use the Chrome Developer tools to drill down to the most relevant elements on the page.\n",
    "\n",
    "This example is simple enough that the URL does not need any special attention.\n",
    "\n",
    "\n",
    "## Fetch the Code\n",
    "\n",
    "The next step is to fetch the HTML code from the website using the Python `requests` library. This is the same library that you used to request data from an API earlier, except in this case we are not fetching JSON data. Instead, we can use the `text` property to get the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awst91JVHz0j"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = ' http://pun.me/pages/funny-jokes.php'\n",
    "response = requests.get(url)\n",
    "\n",
    "# make sure we got a valid response\n",
    "if(response.ok):\n",
    "  # get the full data from the response\n",
    "  data = response.text\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "url = 'https://pun.me/pages/funny-jokes.php'\n",
    "response = requests.get(url)\n",
    "\n",
    "#make sure we get a valid response \n",
    "\n",
    "if response.ok:\n",
    "    data = response.text\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMk_PgxhH11U"
   },
   "source": [
    "## Beautiful Soup\n",
    "\n",
    "Parsing the HTML code that makes up a web page can be quite difficult especially as there is no guarantee that the code is formatted correctly, or consistently with any standard. Web pages are notoriously broken. Your web browser does a heroic job of rendering web pages even when they are broken, so even if you visit a website and it looks fine, that does not mean that the code is actually fine. In addition, to build a polite scraper as defined above requires even more complexity in your code. There are several good libraries that can help you navigate through HTML. We will use [BeatifulSoup](https://www.crummy.com/software/BeautifulSoup/) to simplify the parsing.\n",
    "\n",
    "\n",
    "To use BeautifulSoup we will:\n",
    "\n",
    "* import the library\n",
    "* create an object with the response text \n",
    " \n",
    " \n",
    "BeautifulSoup supports several different HTML parsers. That is, there are different ways that a program may read and understand an HTML page. when creating the object out of the HTML we need to tell BeautifulSoup which parser to use. We will use the default parser to avoid having to install additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cohx0BeLYxWl"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "#soup == objecttype BeautifulSoup-->> object represents the entire HTML document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFbPXDmetfUf"
   },
   "source": [
    "The variable `soup` in this code now contains an object of type *BeautfulSoup*. This object represents the entire HTML document. We can see what it contains with the `prettify()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkmmMuaUufed"
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMazkB1guuAI"
   },
   "source": [
    "## Navigating the Document\n",
    "\n",
    "Now that we have the document parsed, we need to be able to target just the elements of the document that we need. The page contains many different elements, most of which we can ignore. Let's try to find the ones we want. Go back to Chrome Dev tools and find the list of jokes in the HTML. You will notice that each joke is in an `<li>` and together all the `<li>`s  are in an `<ol>` element. Let's try to access that element.\n",
    "\n",
    "To get a list of a certain type of element we can use the `find_all()` method. This is probably the most common method for navigating through a document looking for specific tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('ol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM3fXoa5wpgk"
   },
   "outputs": [],
   "source": [
    "soup.find_all('ol') # make a list of all ol elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnnyEnhUNebO"
   },
   "source": [
    "That did the trick. When we are only expecting a single element we can use the `find()` method instead. Now that we have the list, let's try to get only the `<li>` elements from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list \n",
    "\n",
    "list = soup.find('ol')\n",
    "items = list.find_all('li')\n",
    "print(items )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLQKgeIiUD88"
   },
   "source": [
    "That produces a list of the `<li>` tags. Finally, we want to extract just the text from within those tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = [joke.get_text() for joke in items]\n",
    "print(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ga57EE15URuD",
    "outputId": "64be4f64-cf3a-4e81-efe1-05d4f540a846"
   },
   "outputs": [],
   "source": [
    "jokes = [joke.get_text() for joke in items]\n",
    "print(jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hS42l6OLDHq"
   },
   "source": [
    "So that did the trick. One problem that we may face is if there were multuple `<ol>` elements on the page. In that case, we would want to be as specific as possible. In HTML each element can be given a *class* attribute and an *id* attribute. More than one element may have the same class, but ids are supposed to be unique.\n",
    "\n",
    "A quick examination of the HTML shows that the `<ol>` element has neither a class nor an id. But, the `<div>` element that encloses it has the class *content*. We could select the `<div>` with class *content* then select the `<ol>` from within. Remember that even though this step isn't really necessary for this particular page, we want to make a robust scraper that will work even if the web site owner adds more lists to the same page.\n",
    "\n",
    "As a quick illustration of this lets do exactly what was done above but with a different route to find the jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is what we want from div class content\n",
    "# soup.find_all('div')\n",
    "\n",
    "#</div>, <div style=\"float:left;width:100%;\">\n",
    "#  <div class=\"content\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.find('div', class_= 'content')\n",
    "list = div.find('ol')\n",
    "items = list.find_all('li')\n",
    "jokes = [joke.get_text() for joke in items]\n",
    "print(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "udfz9H50MWs1",
    "outputId": "1e978082-5f84-4055-855d-16893b09a51e"
   },
   "outputs": [],
   "source": [
    "# this gets a list of all divs on the page\n",
    "# soup.find_all('div')\n",
    "\n",
    "# get just the divs with class *content*\n",
    "div = soup.find('div', class_='content')\n",
    "list = div.find('ol')\n",
    "items = list.find_all('li')\n",
    "jokes = [joke.get_text() for joke in items]\n",
    "print(jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5Ej0sgaatND"
   },
   "source": [
    "## Developer Jobs\n",
    "\n",
    "Imagine that you've been tasked with creating a presentation on current trends in developer jobs. Your audience wants to understand things like which technologies are most in demand and what perks employers are using to attract talent. Your preliminary research reveals that the StackOverflow Jobs would be a good data source.\n",
    "\n",
    "This website lists jobs that are targeted at developers and people in the software development community. On the page, there are a number of jobs listed, along with ads and other information. But this site uses pagination to display a long list of jobs one page at a time. To see the second page you can click the Next button at the bottom of the list. To scrape useful information from this website we will need to follow that link.\n",
    "\n",
    "Let’s use the Chrome Developer Tools to examine the page a bit.\n",
    "\n",
    "![Jobs](assets/scraping-3.jpg)\n",
    "\n",
    "Drilling into the page structure you will find that there is a `<div>` element with class `listResults` that contains all the job listings. Each job listing has an HTML structure similar to this:\n",
    "\n",
    "```html\n",
    "<div data-jobid=\"240993\" class=\"-item -job p24 pl48 bb ps-relative bc-black-2 js-dismiss-overlay-container   can-apply \">\n",
    "  <div class=\"dismiss-overlay ps-absolute ta-center p16 t0 r0 b0 l0 grid ai-center jc-center o90 bg-black-050 fs-body3\">\n",
    "    <p class=\"mb0\">\n",
    "      Okay, you won’t see this job anymore. <a href=\"#\" class=\"js-undismiss-job\" data-id=\"240993\">Undo</a>\n",
    "    </p>\n",
    "  </div>\n",
    "  <div class=\"dismiss-trigger js-dismiss-job ps-absolute r12 fc-black-500 c-pointer\" data-id=\"240993\" data-referrer=\"JobSearch\" title=\"Dismiss job\"><svg aria-hidden=\"true\" class=\"svg-icon iconClearSm\" width=\"14\" height=\"14\" viewBox=\"0 0 14 14\"><path d=\"M12 3.41L10.59 2 7 5.59 3.41 2 2 3.41 5.59 7 2 10.59 3.41 12 7 8.41 10.59 12 12 10.59 8.41 7z\"></path></svg></div>\n",
    "  <div class=\"-job-summary\">\n",
    "    <div class=\"-title\">\n",
    "      <span data-href=\"https://stackoverflow.com/users/signup?returnUrl=%2fjobs%2fset-favorite%2f240993%3freturnUrl%3d%252Fjobs%253Fmed%253Dsite-ui%2526ref%253Djobs-tab%26referrer%3dJobSearch%26sec%3dFalse&amp;ssrc=jobs\" data-jobid=\"240993\" class=\"fav-toggle ps-absolute l16 c-pointer js-fav-toggle \" title=\"Click to add this job to your favorites.\" data-ga-label=\"Toptal | Front-End Developer @ PUB Team | 240993\" )=\"\">\n",
    "        <svg aria-hidden=\"true\" class=\"svg-icon iconStar\" width=\"18\" height=\"18\" viewBox=\"0 0 18 18\"><path d=\"M9 12.65l-5.29 3.63 1.82-6.15L.44 6.22l6.42-.17L9 0l2.14 6.05 6.42.17-5.1 3.9 1.83 6.16z\"></path></svg>\n",
    "      </span>\n",
    "      <h2 class=\"fs-subheading job-details__spaced mb4\">\n",
    "        <a href=\"/jobs/240993/front-end-developer-pub-team-toptal?a=1iOW90N7AGru&amp;so=i&amp;pg=1&amp;offset=24&amp;total=965\" title=\"Front-End Developer @ PUB Team\" class=\"s-link s-link__visited\">Front-End Developer @ PUB Team</a>        \n",
    "      </h2>\n",
    "      <span class=\"ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t24\">5d ago</span>\n",
    "    </div>\n",
    "    <div class=\"fc-black-700 fs-body2 -company\">\n",
    "        <span>Toptal</span>\n",
    "        <span class=\"fc-black-500\">\n",
    "             - No office location        \n",
    "        </span>\n",
    "    </div>\n",
    "    <div class=\"mt2 -perks\">\n",
    "      <span class=\"-remote pr16\">Remote</span>\n",
    "    </div>\n",
    "    <div class=\"mt12 -tags\">\n",
    "      <a href=\"/jobs/developer-jobs-using-javascript\" class=\"post-tag job-link no-tag-menu\">javascript</a>\n",
    "      <a href=\"/jobs/developer-jobs-using-node.js\" class=\"post-tag job-link no-tag-menu\">node.js</a>\n",
    "      <a href=\"/jobs/developer-jobs-using-css\" class=\"post-tag job-link no-tag-menu\">css</a>\n",
    "      <a href=\"/jobs/developer-jobs-using-html\" class=\"post-tag job-link no-tag-menu\">html</a>\n",
    "      <a href=\"/jobs/developer-jobs-using-reactjs\" class=\"post-tag job-link no-tag-menu\">reactjs</a>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "So there is still a lot of extraneous information in there. We're only really interested in the job title, the company, the list of technologies, and the perks. Let's use BeautifulSoup to try to select the parts that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Article Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@nveenverma/web-scraping-tutorial-project-scraping-stack-overflow-e28bb139fc3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style \n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# soup = BeautifulSoup(data, 'html.parser')\n",
    "style.use('fivethirtyeight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://stackoverflow.com/tags'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(response.content, 'html.parser')\n",
    "\n",
    "body = soup.find('body')\n",
    "\n",
    "type(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tags = body.find_all('a', class_ = 'post-tag')\n",
    "languages = [i.text for i in lang_tags]\n",
    "languages[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = body.find_all('span', class_ = 'item-multiplier-count')\n",
    "noTags= [int(i.text) for i in tag_counts]\n",
    "listed = list(zip(languages, noTags))\n",
    "listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join the two lists in a dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check, if there is any error in length of the extracted bs4 object\n",
    "\n",
    "def error_checking(list_name, length):\n",
    "    if (len(list_name) != length):\n",
    "        print('Error in {} parsing, length not equal to {}!!!'.format(list_name, length))\n",
    "        return -1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def get_top_languages(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #Parsing html data using Beautiful Soup\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    \n",
    "    #Extracting top languages \n",
    "    lang_tags = body.find_all('a', class_ = 'post-tag')\n",
    "    error_checking(lang_tags, 36)\n",
    "    languages = [i.text for i in lang_tags]\n",
    "    \n",
    "    #Extracting tag Counts\n",
    "    tag_counts = body.find_all('span', class_ = 'item-multiplier-count')\n",
    "    error_checking(tag_counts, 36)\n",
    "    no_of_tags = [int(i.text) for i in tag_counts]\n",
    "    \n",
    "    # Join 2 lists together\n",
    "    \n",
    "    df = pd.DataFrame({'Languages':languages, 'Tag Count': no_of_tags})\n",
    "\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1 = 'https://stackoverflow.com/tags'\n",
    "\n",
    "df = get_top_languages(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.bar(height=df['Tag Count'][:10], x = df['Languages'][:10])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('Languages')\n",
    "plt.savefig('lang_vs_tag_counts.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Download Webpage from stackoverflow\n",
    "Parse the document content into BeautifulSoup\n",
    "Extract Top Questions\n",
    "Extract their respective Summary\n",
    "Extract their respective Tags\n",
    "Extract their respective no. of votes, answers and views\n",
    "Put all code togther and join the lists\n",
    "Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p a — finds all a tags inside of a p tag.\n",
    "# Selecting a tags inside p tag\n",
    "soup.select('p a')\n",
    "div.outer-text - finds all div tags with a class of outer-text.\n",
    "div#first - finds all div tags with an id of first.\n",
    "body p.outer-text - finds any p tags with a class of outer-text inside of a body tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://stackoverflow.com/questions?sort=votes&pagesize=50'\n",
    "response1 = requests.get(url)\n",
    "response1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing html data using Beautiful Soup\n",
    "\n",
    "soup1 = bs(response1.content , 'html.parser')\n",
    "\n",
    "#body1\n",
    "body1 = soup1.select_one('body')\n",
    "\n",
    "#\n",
    "type(body1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the question is inside a tag, which has a class of question-hyperlink\n",
    "# Taking cue from our previous Goal, we can use this class along with a tag, to extract all the question links in a list. \n",
    "# However, there are more question hyperlinks in sidebar which will also be extracted in this case. \n",
    "#To avoid this scenario, we can combine a tag, question-hyperlink class with their parent h3 tag. This will give us exactly 50 Tags.\n",
    "question_links = body1.select(\"h3 a.question-hyperlink\")\n",
    "\n",
    "questions = [i.text for i in question_links]\n",
    "questions[:2]\n",
    "\n",
    "# q_tags = body.find_all('a', class_ = 'post-tag')\n",
    "# questions = [i.text for i in q_tags]\n",
    "# questions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_divs= body1.select(\"div.excerpt\")\n",
    "summary_divs[0]\n",
    "summaries = [i.text.strip() for i in summary_divs]\n",
    "# summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracting tags per question is the most complex task in this post. \n",
    "Here, we cannot find unique class or id for each tag, and there are multiple tags per question that we need to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 using 3rrd child overall and second div child of above extracted object. Use nth-of-type() method \n",
    "tag_divs = body1.select(\"div.summary > div:nth-of-type(2)\")\n",
    "tag_divs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Now, we can use list comprehension to extract a tags in a list, grouped per question.\n",
    "\n",
    "a_tags_list = [i.select('a') for i in tag_divs]\n",
    "\n",
    "#Printing first question's a tags\n",
    "a_tags_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Now we will run a for loop for going through each question and use list comprehension inside it, to extract the tags names.\n",
    "\n",
    "tags = []\n",
    "\n",
    "for a_group in a_tags_list:\n",
    "    tags.append([a.text for a in a_group])\n",
    "    \n",
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Votes...>>>>> Theycan be found by using span tag along with vote-count-post class and nested strong tags (refer Figure 2.).\n",
    "\n",
    "vote_spans = body1.select(\"span.vote-count-post strong\")\n",
    "print(vote_spans[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_votes = [int(i.text) for i in vote_spans]\n",
    "no_of_votes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Answers They can be found by using div tag along with status class and nested strong tags. \n",
    "#Here, we don't use answered-accepted because its not common among all questions,\n",
    "# few of them (whose answer are not accepted) have the class - answered (refer Figure 2.)\n",
    "\n",
    "answer_divs = body1.select(\"div.status strong\")\n",
    "answer_divs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_answers = [int(i.text) for i in answer_divs]\n",
    "no_of_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of Views\n",
    "answer_views = body1.select(\"div.supernova\")\n",
    "answer_views[0]\n",
    "# now we need to clean string and turn into an integer format \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_views = [i['title'] for i in answer_views]\n",
    "no_of_views = [i[:-6].replace(',','') for i in no_of_views]\n",
    "no_of_views = [int(i) for i in no_of_views]\n",
    "no_of_views[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling a DataFrame of StackOverflow All Questions Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_questions(url, question_count):\n",
    "    # WARNING: Only enter one of these 3 values [15, 30, 50].\n",
    "    # Since, stackoverflow, doesn't display any other size questions list\n",
    "    url = url + \"?sort=votes&pagesize={}\".format(question_count)\n",
    "    \n",
    "    #Using requests module for downloading webpage content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parsing html data\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    \n",
    "    # Extracting Top Questions\n",
    "    question_links = body1.select(\"h3 a.question-hyperlink\")\n",
    "    error_checking(question_links, question_count)\n",
    "    questions = [i.text for i in question_links]\n",
    "    \n",
    "    #Extracting Tags\n",
    "    tags_div = body1.select(\"div.summary > div:nth-of-type(2)\")\n",
    "    \n",
    "    error_checking(tags_div, question_count)\n",
    "    a_tags_list = [i.select('a') for i in tags_div]\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    for a_group in a_tags_list:\n",
    "        tags.append([a.text for a in a_group])\n",
    "        \n",
    "    # Extracting Number of Votes\n",
    "    vote_spans = body1.select(\"span.vote-count-post strong\")\n",
    "    error_checking(vote_spans,question_count)\n",
    "    no_of_votes = [int(i.text) for i in vote_spans]\n",
    "    \n",
    "    \n",
    "    #Extracting Number of answers \n",
    "    answer_divs = body1.select(\"div.status strong\")\n",
    "    error_checking(answer_divs, question_count)\n",
    "    no_of_answers = [int(i.text) for i in answer_divs]\n",
    "    \n",
    "    # Extracting Number of Views\n",
    "    \n",
    "    div_views = body1.select(\"div.supernova\")\n",
    "    error_checking(div_views, question_count)\n",
    "    \n",
    "    no_of_views = [i['title'] for i in answer_views]\n",
    "    no_of_views = [i[:-6].replace(',','') for i in no_of_views]\n",
    "    no_of_views = [int(i) for i in no_of_views] \n",
    "    \n",
    "    # Putting them all together \n",
    "    \n",
    "    df = pd.DataFrame({'question':questions, 'summary':summaries,\n",
    "                      'tags':tags, 'no_of_votes': no_of_votes,\n",
    "                      'no_of_answers': no_of_answers, 'no_of_views': no_of_views})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL2 = 'https://stackoverflow.com/questions'\n",
    "\n",
    "df1 = get_top_questions(URL2, 50)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3,1, figsize = (12,8))\n",
    "ax[0].bar(df1.index, df1.no_of_votes)\n",
    "ax[0].set_ylabel('No of Votes')\n",
    "\n",
    "ax[1].bar(df1.index, df1.no_of_views)\n",
    "ax[1].set_ylabel('No of Views')\n",
    "\n",
    "ax[2].bar(df1.index, df1.no_of_answers)\n",
    "ax[2].set_ylabel('No of Answers')\n",
    "\n",
    "plt.savefig('votes_vs_views_vs_answers.png', bbox_inches ='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://stackoverflow.com/jobs'\n",
    "response = requests.get(url)\n",
    "\n",
    "if (response.ok):\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    # find all the elements with class *-job-summary*\n",
    "    summary = soup.select('.-job-summary > .-company > span')\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://stackoverflow.com/jobs'\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(response.text, 'html.parser')\n",
    "jobs_list = soup.find_all('a', class_= 's-link stretched-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Software Engineer',\n",
       " 'Sitecore Developer (C#, JavaScript, HTML, CSS)',\n",
       " 'Java Developer with Kafka',\n",
       " 'Lead Software Engineer - C#, Java, Node.js, JavaScript',\n",
       " 'Senior Software Engineer',\n",
       " 'Lead or Senior Software Engineer',\n",
       " 'Lead or Senior Software Engineer',\n",
       " 'Full Stack Software Engineer',\n",
       " 'Full Stack Developer',\n",
       " 'Frontend Entwickler Angular (m/w/d)',\n",
       " 'Senior Software Engineer',\n",
       " 'Senior Software Engineer (Frome)',\n",
       " 'Full Stack Software Engineer',\n",
       " 'Full-Stack Web Developer',\n",
       " 'Arquitecto /a de Software',\n",
       " 'FULL STACK DEVELOPER (M/W/D)',\n",
       " 'Full Stack Application Developer',\n",
       " 'Java Developer',\n",
       " 'Full Stack Software Engineer',\n",
       " 'Software Engineer Lead',\n",
       " 'Software Engineer (React / Node.js)',\n",
       " 'Python Developer (Onsite in Paris, or Remote in France, UK, Germany)',\n",
       " 'Web Developer Full Stack (H/F)',\n",
       " 'Maquetador / Programador Junior',\n",
       " 'Full Stack Software Engineer (f/m/d)']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = [job.text for job in jobs_list]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gWgN0v4oaH6"
   },
   "outputs": [],
   "source": [
    "url = 'https://stackoverflow.com/jobs'\n",
    "response = requests.get(url)\n",
    "\n",
    "# make sure we got a valid response\n",
    "if(response.ok):\n",
    "  # get the full data from the response\n",
    "  data = response.text\n",
    "  soup = BeautifulSoup(data, 'html.parser')\n",
    "  \n",
    "  # find all elements with class *-job-summary*\n",
    "  summary =soup.find_all(class_='-job-summary')\n",
    "  #print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRbvScK8psdD"
   },
   "source": [
    "Not bad! That gave us a list of the divs containing job info. But, that is still a lot of extraneous HTML. We could narrow it down further by just geting the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlMKK2vop6vc"
   },
   "outputs": [],
   "source": [
    "titles = soup.find_all(class_='title')\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIRBqf6py776"
   },
   "source": [
    "Better, but the actual title text that we want is in an `<a>` tag inside an `<h2>` tag. We can continue navigating down the document or we can use another technique: [**CSS Selectors**](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors). CSS is the language used to apply styles to web pages. CSS selectors are used to select specific elements on the page so that styles may be applied to just those elements. But the syntax is so powerful that we can use the same selector syntax in BeautifulSoup. \n",
    "\n",
    "To select all elements with a particular class use the `.classname` selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgCHCBtFz4Nz"
   },
   "outputs": [],
   "source": [
    "titles = soup.select('.-title')\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUu0O4620BXC"
   },
   "source": [
    "This gives us the same output as earlier. So let's extend this a bit. To select only the `<h2>` elements in the divs with class *-title*, we can use the **child** selector. The `>` symbol is used to specify a child element. A child element is an element contained within another element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "aa0BAJEk0rUQ",
    "outputId": "31d9143f-8699-4f17-c783-89156deef55a"
   },
   "outputs": [],
   "source": [
    "title_headings = soup.select('.-title > h2')\n",
    "print(title_headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clDWlNML07me"
   },
   "source": [
    "Okay. That's better, but it'd be even better if we selected only the `<a>` tags `<h2>`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "G245tlUc1Gp-",
    "outputId": "ac12b519-bdcb-46c2-dade-5d3d7193b2e2"
   },
   "outputs": [],
   "source": [
    "title_links = soup.select('.-title > h2 > a')\n",
    "print(title_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4XvZfYw1bS0"
   },
   "source": [
    "Now, we can use `get_text()` as we did above to extract the text content of the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eIWVImrs1jaL",
    "outputId": "edc0a334-ff92-421c-f010-1b1fb7fd477a"
   },
   "outputs": [],
   "source": [
    "jobs = [job.get_text() for job in title_links]\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GZyCPIZ8t_Z"
   },
   "source": [
    "How would we get a list of the companies that posted jobs on this page? Take a few minutes and try to construct the CSS selector expression that will return this information.\n",
    "\n",
    "It may be obvious from the HTML that the company information is inside the job summary in a `<div>` with class `-company`. If you select the `<span>`s content, you will get the company information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "xSm1IiKy88lU",
    "outputId": "a3d7199f-3525-46f5-b367-38f7eb89622d"
   },
   "outputs": [],
   "source": [
    "company_spans = soup.select('.-job-summary > .-company > span')\n",
    "companies = [company.get_text() for company in company_spans]\n",
    "print(companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aT94Zr9n9ZP9"
   },
   "source": [
    "We are getting the information that we want, but there's extra text here that we do not want. Look carefully at the div with class *-company* and notice that it actually contain multiple spans. The company name is usually the first one. We can use the `nth-of-type()` selector to, say, only grab me the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XO8cuy8J91gC",
    "outputId": "51a1a9e8-4df2-464f-a32e-c3c3104ccbc1"
   },
   "outputs": [],
   "source": [
    "company_spans = soup.select('.-job-summary > .-company > span:nth-of-type(1)')\n",
    "companies = [company.get_text() for company in company_spans]\n",
    "print(companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14z8vYZx-s4u"
   },
   "source": [
    "Getting the tags for each job is much the same. Let's try to put together some code that will get all the information about the job that we need and put it into a dictionary object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aKgCipzk_TLx",
    "outputId": "e5ff6055-aa48-4bfe-f4a2-6d6c62ef4c1c"
   },
   "outputs": [],
   "source": [
    "# iterate over all jobs\n",
    "raw_jobs = soup.select('.-job-summary')\n",
    "jobs = []\n",
    "for job in raw_jobs:\n",
    "  title = job.select_one('.-title > h2 > a').get_text() # extract the title\n",
    "  company = job.select_one('.-company > span:nth-of-type(1)').get_text().strip() # extract the company\n",
    "  tags = [tag.get_text() for tag in job.select('.-tags a')] # extract a list of tags\n",
    "  job = {'title': title, 'company': company, 'tags': tags} # construct a dictionary\n",
    "  jobs.append(job) # add dictionary to list\n",
    "print(jobs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NVuOQQnB6nd"
   },
   "source": [
    "## Following a link\n",
    "Now that we can scrape the list of jobs from the first page, what about the other pages? Let's look at the HTML code for the next button.\n",
    "\n",
    "```html\n",
    "<a href=\"/jobs?sort=i&pg=2\" title=\"page 2 of 39\" class=\"prev-next job-link test-pagination-next\">\n",
    "  <span class=\"text\">next</span>\n",
    "  <i class=\"material-icons\">chevron_right</i>\n",
    "</a>\n",
    "```\n",
    "\n",
    "The next button is made up of a clickable link styled to look like a button. The `href` attribute of the link contains the URL for the next page. The URL of this button is `/jobs?sort=i&pg=2`. Notice that this is only part of the full URL for that page. A URL is made up of the domain name and path and optional query string. Since we are on the `https://stackoverflow.com/jobs` website, the browser knows that if we click this link we want to remain on the same website and just visit a different path. So the actual request will be sent to `https://stackoverflow.com/jobs?sort=i&pg=2`. The bit after the `?` is called the query string, and it is one way to send data to the server describing what you are requesting. It appears that this link will accept a parameter named `pg` representing the page number. If you click on the next button several times you will notice that the bit at the end of the URL that says `pg=2` changes to `pg=3` then `pg=4` and so on. \n",
    "\n",
    "That means that we can modify the URL by incrementing the page number to request more jobs. We can continue to do so until there are no more pages. \n",
    "\n",
    "To set the query string on the request URL, we can use the `params` parameter on the get method. For example,\n",
    "\n",
    "```python\n",
    "page_number = 1\n",
    "query = {'sort':'i', 'pg': page_number}\n",
    "url = 'https://stackoverflow.com/jobs'\n",
    "response = requests.get(url, params=query)\n",
    "```\n",
    "\n",
    "We could then update the `page_number` variable and repeat this call.\n",
    "\n",
    "This raises a few questions:\n",
    "\n",
    " - How do we know when there are no more pages?\n",
    " - Of we rapidly send many requests for many pages to the server, will we overwhelm the server and possibly get ourselves blocked?\n",
    " \n",
    "There may be several answers to the first question. If you go to the website and visit the last page of results you will see that the next button is not displayed on that page. So the absence of the next button could be the condition on which we stop the processing. There may be others that you can readily spot but we will go with that option for now.\n",
    "\n",
    "The second one does not rely on the HTML structure of the page, but rather on the way we code up the solution. The above code would process a page of jobs and create a list of dictionaries from that page. To repeat that we need some sort of loop that will iterate over all pages and for each page perform something similar to the code that we already wrote. When we get to the last page we stop the loop and at that point we will have a list of all jobs on the website. The problem here is that our program will process each page much more rapidly than a human browsing the website would. If we're not careful, this could appear to be a denial of service attack against the website, and since most web servers can identify such attacks and take steps to protect themselves, we want to avoid this behavior. \n",
    "\n",
    "One solution is to deliberately slow down the rate of requests that we make. After making one request, we can wait a few seconds before making another one. If we wait 1 second between requests and we need to request 500 pages, then that is at least 500 seconds for our program to run. That is not so bad since hopefully scraping the data is a one time affair. There are some libraries that help to moderate your request rate if you are buiding a more sofisticated scraper. For our simple scraper, we can use the Python's `sleep()` method to pause execution for a short time.\n",
    "\n",
    "## Some other considerations\n",
    "\n",
    "Before we jump into code, there are a few other considerations that we should make since we are requesting multiple pages. We should clearly identify ourselves to the web server. That is, when your browser makes a request to a web server it sends along a header named **User-Agent** with a value that identifies the browser itself. For instance, in Chrome the value looks like this:\n",
    "\n",
    "```\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\n",
    "```\n",
    "\n",
    "And in Firefox the value looks like this:\n",
    "\n",
    "```\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:63.0) Gecko/20100101 Firefox/63.0\n",
    "```\n",
    "\n",
    "This way the web server can track how many visitors use a particular browser. We can set a value that identifies us and at least provide a contact. If the website owners wish, they may contact us to get some clarity on why we are scraping data from their website and maybe negotiate a better experience for themselves as well as us. We could provide a value like:\n",
    "\n",
    "```\n",
    "'jobscraper - school project (yourname@gmail.com)'\n",
    "```\n",
    "\n",
    "For example, when making the request we could set the header like this:\n",
    "\n",
    "```\n",
    "url = '...'\n",
    "headers = {'user-agent': 'jobscraper - school project (yourname@gmail.com)'}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "Another consideration is monitoring the program as it runs. We are creating a program that may take 10 to 20 minutes or even more to run. It would be important to \"see\" what is happening as the program runs. This helps us to follow the progress, ensure the  program isn't hung up, and to detect problems early. To this end, we can simply print the number of requests that have been made so far, the frequency of the requests, the number of items found so far and any errors encountered.\n",
    "\n",
    "Putting all of this together can be a bit daunting, but careful step-by-step thought will make it possible. A complete working program is given below. Before you run it, ensure you set your email address in the header. Also, notice the variable `MAX_REQUESTS` which is used to set an upper limit on the number of requests made. You may adjust this value as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q2P1WzaCtM0o",
    "outputId": "d64453d3-432e-4de5-8e3d-a9f18f71697e"
   },
   "outputs": [],
   "source": [
    "# import all dependencies at the top\n",
    "from time import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "# define a function to process the page\n",
    "def process_page(soup, jobs):  \n",
    "  \n",
    "  # find all elements with class *-job-summary*\n",
    "  raw_jobs = soup.select('.-job-summary')\n",
    "\n",
    "  # same as above, extract the info we need\n",
    "  for job in raw_jobs:\n",
    "    title = job.select_one('.-title > h2 > a').get_text() # extract the title\n",
    "    company = job.select_one('.-company > span:nth-of-type(1)').get_text().strip() # extract the company\n",
    "    tags = [tag.get_text() for tag in job.select('.-tags a')] # extract a list of tags\n",
    "    job = {'title': title, 'company': company, 'tags': tags} # construct a dictionary\n",
    "    jobs.append(job) # add dictionary to list\n",
    "\n",
    "    \n",
    "# prepare for the monitoring logic\n",
    "start_time = time() # note the system time when the program starts\n",
    "request_count = 0 # track the number of requests made\n",
    "\n",
    "# create a list to store the data in\n",
    "jobs = []\n",
    "\n",
    "# variables to handle the request loop\n",
    "has_next_page = True\n",
    "MAX_REQUESTS = 100 # do not request more than 100 pages\n",
    "page_number = 1\n",
    "query = {'sort':'i', 'pg': page_number}\n",
    "url = 'https://stackoverflow.com/jobs'\n",
    "headers = {'user-agent': 'jobscraper - school project (myeamail@gmail.com)'}\n",
    "\n",
    "while has_next_page and request_count <= MAX_REQUESTS:\n",
    "  # keep the output clear\n",
    "  clear_output(wait = True)\n",
    "  \n",
    "  # make an initial request\n",
    "  response = requests.get(url, params=query, headers=headers)\n",
    "\n",
    "  # make sure we got a valid response\n",
    "  if(response.ok):\n",
    "    # get the full data from the response\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    process_page(soup, jobs)\n",
    "\n",
    "    # check for the next page\n",
    "    # look for the presence of element with class *test-pagination-next*\n",
    "    next_button = soup.select('.test-pagination-next')\n",
    "    has_next_page = len(next_button) > 0\n",
    "    \n",
    "  else:\n",
    "    # display a warning if there are any problems\n",
    "    warn('Request #: {}, Failed with status code: {}'.format(request_count, response.status_code))\n",
    "  \n",
    "  request_count += 1\n",
    "  \n",
    "  # go to sleep for a bit\n",
    "  # we use a random number between 1 and 5 so\n",
    "  # We can wait as long as 5 seconds to make a second request\n",
    "  \n",
    "  sleep(randint(1,3))\n",
    "  \n",
    "  # output some logs for monitoring\n",
    "  elapsed_time = time() - start_time\n",
    "  print('Requests: {}, Frequency: {} requests/s, {} jobs processed.'.format(request_count, request_count/elapsed_time, len(jobs)))\n",
    "  \n",
    "  # prepare for next iteration\n",
    "  page_number += 1\n",
    "      \n",
    "print('Sraping complete')\n",
    "print('Requests: {}, Frequency: {} requests/s, {} jobs processed.'.format(request_count, request_count/elapsed_time, len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "x9HKAANNO6Sq",
    "outputId": "5029ccc0-e5ba-4441-9936-d88778fc86c7"
   },
   "outputs": [],
   "source": [
    "# print the first five jobs\n",
    "jobs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIqwqsFMUKLO"
   },
   "source": [
    "## Save to a file\n",
    "Getting the data from the server is only the first part of the problem. The next step is to persist the data in some format that makes it available for analysis later. A CSV file is a fairly common format and well supported in many environments. The `csv` Python library provides the tools to read and write csv files in Python. \n",
    "\n",
    "## Note about Colab\n",
    "\n",
    "Since this is a hosted service writing a file to the filesystem is not quite as straightforward as if this code was running on your local machine. We can get around this by writing to your Google Drive. In order to use Google Drive, you will need to be authenticated. Luckily, Google provides a library named *PyDrive* that makes this simple.\n",
    "\n",
    "First, ensure that PyDrive is installed in the environment. This command only needs to be executed once in a notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fU5Y5i8ncQh1",
    "outputId": "24e6f33f-9bfc-4321-c053-b0a529b263d4"
   },
   "outputs": [],
   "source": [
    "# install this library\n",
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJGcIGYevvE8"
   },
   "source": [
    "To authenticate with Google, do the following. Note that you will be prompted to login with your Google credentials and then you will be given a code to enter. When you enter that code, this notebook will have permission to write files to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDNREOAgwJzx"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once in a notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fgr6nQ0QwP3g"
   },
   "source": [
    "Next, we can create a csv string of our jobs and write it to Google Drive with a file  name of \"jobs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tGQPgHvaWnMR",
    "outputId": "2f14769a-4252-415e-d856-22335755a716"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "\n",
    "# Create an output stream\n",
    "output = io.StringIO()\n",
    "\n",
    "# these are the names of the properties in the dictionary\n",
    "fieldnames = ['title', 'company', 'tags']\n",
    "\n",
    "# create a writer object, it can write dictionaries to the output stream\n",
    "writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "\n",
    "# write all the headings \n",
    "writer.writeheader()\n",
    "\n",
    "# iterate the jobs and write each one\n",
    "for job in jobs:\n",
    "  writer.writerow(job)\n",
    "\n",
    "\n",
    "# Create & upload a text file.\n",
    "uploaded = drive.CreateFile({'title': 'jobs.csv'})\n",
    "uploaded.SetContentString(output.getvalue())\n",
    "uploaded.Upload()\n",
    "print('Uploaded file with ID {}'.format(uploaded.get('id')))  \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbIZbsOewy73"
   },
   "source": [
    "Visit your Google Drive and find the file named \"jobs.csv\", and verify that it contains a list of jobs that was scraped from the StackOverflow jobs site."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "python_web_scraping.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
